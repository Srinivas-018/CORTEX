"""
Large File Upload Page - S3 Direct Multipart Upload Demo

This Streamlit page demonstrates the end-to-end flow for uploading large files
(up to 1 TB) directly to S3 using multipart uploads with presigned URLs.

Features:
    - Direct browser-to-S3 uploads (files don't pass through Streamlit server)
    - Support for files up to 1 TB
    - Progress tracking and upload statistics
    - Fallback to standard upload for smaller files (<100 MB)
    - Verification of upload completion
    - Integration with CORTEX forensic case management

Architecture:
    1. User enters file details (name, size)
    2. FastAPI backend creates multipart upload and generates presigned URLs
    3. Browser uploads parts directly to S3 using embedded HTML/JS component
    4. Browser calls FastAPI to complete the multipart upload
    5. Streamlit verifies object exists in S3 and shows success

Configuration:
    Set S3 configuration in the page or via environment variables:
    - S3_BUCKET: S3 bucket name
    - S3_REGION: AWS region
    - S3_KEY_PREFIX: Prefix for uploaded objects (e.g., 'evidence/')
    - BACKEND_URL: FastAPI service URL (default: http://localhost:8001)

Usage:
    This page can be accessed via Streamlit's multi-page app navigation:
    - Navigate to "Large File Upload" in the sidebar
    - Or access directly at http://localhost:8501/large_upload

Requirements:
    - FastAPI companion service running (server/s3_complete_app.py)
    - AWS credentials configured
    - S3 bucket with appropriate permissions and CORS configuration

See README.md for detailed setup instructions.
"""

import streamlit as st
import os
from pathlib import Path
import boto3
from botocore.exceptions import ClientError
import sys

# Add parent directory to path to import modules
sys.path.insert(0, str(Path(__file__).parent.parent))

from modules.s3_multipart import (
    calculate_part_size,
    calculate_part_count,
    verify_object_exists
)
from modules.image_input import save_uploaded_file_to_disk, calculate_hash_file


# Page configuration
st.set_page_config(
    page_title="Large File Upload - CORTEX",
    page_icon="üì§",
    layout="wide"
)


def get_s3_config():
    """
    Get S3 configuration from environment or defaults.
    
    In production, these should be set via environment variables or
    a secure configuration management system.
    """
    return {
        'bucket': os.getenv('S3_BUCKET', 'cortex-forensics-bucket'),
        'region': os.getenv('S3_REGION', 'us-east-1'),
        'key_prefix': os.getenv('S3_KEY_PREFIX', 'evidence/'),
        'backend_url': os.getenv('BACKEND_URL', 'http://localhost:8001')
    }


def render_configuration_panel():
    """Render S3 configuration panel"""
    with st.expander("‚öôÔ∏è S3 Configuration", expanded=False):
        st.write("**Current Configuration:**")
        
        config = get_s3_config()
        
        col1, col2 = st.columns(2)
        with col1:
            st.write(f"**Bucket:** `{config['bucket']}`")
            st.write(f"**Region:** `{config['region']}`")
        with col2:
            st.write(f"**Key Prefix:** `{config['key_prefix']}`")
            st.write(f"**Backend URL:** `{config['backend_url']}`")
        
        st.info("""
        üí° **Configuration Options:**
        - Set environment variables: `S3_BUCKET`, `S3_REGION`, `S3_KEY_PREFIX`, `BACKEND_URL`
        - Or edit `get_s3_config()` in this file for defaults
        - Ensure FastAPI service is running at the backend URL
        """)
        
        # Health check
        try:
            import requests
            response = requests.get(f"{config['backend_url']}/health", timeout=2)
            if response.status_code == 200:
                st.success("‚úÖ Backend service is healthy")
            else:
                st.warning("‚ö†Ô∏è Backend service responded but may not be healthy")
        except Exception as e:
            st.error(f"‚ùå Cannot reach backend service: {str(e)}")
            st.write("**To start the backend service:**")
            st.code("uvicorn server.s3_complete_app:app --host 0.0.0.0 --port 8001", language="bash")


def render_s3_direct_upload_demo():
    """Render the S3 direct upload demonstration"""
    st.header("üì§ S3 Direct Multipart Upload")
    
    st.markdown("""
    This page demonstrates direct browser-to-S3 uploads for large files (up to 1 TB).
    Files are uploaded directly from your browser to S3 without passing through the
    Streamlit server, avoiding memory and timeout issues.
    """)
    
    # Configuration panel
    render_configuration_panel()
    
    st.divider()
    
    # Upload mode selection
    upload_mode = st.radio(
        "Select Upload Mode:",
        ["üöÄ Direct S3 Upload (for large files)", "üìÅ Standard Upload (for files < 100 MB)"],
        help="Use Direct S3 Upload for files over 100 MB. Standard Upload is simpler but limited by server memory."
    )
    
    if upload_mode.startswith("üöÄ"):
        render_direct_upload_interface()
    else:
        render_standard_upload_interface()


def render_direct_upload_interface():
    """Render the direct S3 upload interface"""
    st.subheader("Direct S3 Multipart Upload")
    
    st.info("""
    **How it works:**
    1. Enter your file details below
    2. Click "Initialize Upload" to get presigned URLs
    3. Select your file when prompted by the upload component
    4. The file will be uploaded directly to S3 from your browser
    5. Verify the upload completed successfully
    """)
    
    config = get_s3_config()
    
    # File details form
    with st.form("file_details_form"):
        col1, col2 = st.columns(2)
        
        with col1:
            file_name = st.text_input(
                "File Name",
                placeholder="device-image.img",
                help="Name of the file to upload"
            )
            
            file_size_gb = st.number_input(
                "File Size (GB)",
                min_value=0.001,
                max_value=1024.0,
                value=1.0,
                step=0.1,
                help="Size of your file in gigabytes"
            )
        
        with col2:
            content_type = st.selectbox(
                "Content Type",
                [
                    "application/octet-stream",
                    "image/disk-image",
                    "application/x-raw-disk-image"
                ],
                help="MIME type of the file"
            )
            
            case_id = st.text_input(
                "Case ID (optional)",
                placeholder="CASE-2025-001",
                help="Associate with a forensic case"
            )
        
        submit = st.form_submit_button("üöÄ Initialize Upload", type="primary")
    
    if submit and file_name:
        file_size_bytes = int(file_size_gb * 1024 * 1024 * 1024)
        
        # Calculate upload details
        part_size = calculate_part_size(file_size_bytes)
        part_count = calculate_part_count(file_size_bytes, part_size)
        
        st.write("**Upload Details:**")
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("File Size", f"{file_size_gb:.2f} GB")
        with col2:
            st.metric("Part Size", f"{part_size / (1024*1024):.1f} MB")
        with col3:
            st.metric("Total Parts", f"{part_count:,}")
        with col4:
            estimated_time_min = (file_size_bytes / (1024*1024)) / 10  # Assume 10 MB/s
            st.metric("Est. Time", f"{estimated_time_min:.0f} min")
        
        if part_count > 10000:
            st.error(f"‚ùå File too large: {part_count} parts exceeds S3 limit of 10,000 parts")
            st.write("Consider splitting the file or increasing the part size.")
            return
        
        st.divider()
        
        # Show next steps
        st.warning("""
        ‚ö†Ô∏è **Note:** This is a demonstration page. The actual upload component integration
        requires the full HTML/JS component to be embedded here.
        
        **Next Steps:**
        1. Ensure FastAPI backend is running
        2. The upload component will appear here in the full implementation
        3. You will be able to select your file and upload directly to S3
        
        **For now, you can test the backend directly:**
        ```bash
        curl -X POST http://localhost:8001/create-multipart \\
          -H "Content-Type: application/json" \\
          -d '{
            "bucket": "%s",
            "key": "%s%s",
            "file_size": %d,
            "content_type": "%s"
          }'
        ```
        """ % (config['bucket'], config['key_prefix'], file_name, file_size_bytes, content_type))
        
        # In a full implementation, you would call:
        # from modules.s3_streamlit import render_s3_direct_upload
        # result = render_s3_direct_upload(config, ...)
        
    elif submit and not file_name:
        st.error("‚ùå Please enter a file name")


def render_standard_upload_interface():
    """Render the standard upload interface (for smaller files)"""
    st.subheader("Standard File Upload")
    
    st.info("""
    **Standard Upload** is suitable for files under 100 MB. The file is uploaded
    through the Streamlit server and then can be processed or stored.
    
    For larger files, use the Direct S3 Upload mode above.
    """)
    
    uploaded_file = st.file_uploader(
        "Select File",
        type=['img', 'bin', 'dd', 'raw', 'e01'],
        help="Upload a file up to 100 MB"
    )
    
    if uploaded_file is not None:
        file_size_mb = uploaded_file.size / (1024 * 1024)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**File Information:**")
            st.write(f"**Name:** {uploaded_file.name}")
            st.write(f"**Size:** {file_size_mb:.2f} MB")
            st.write(f"**Type:** {uploaded_file.type or 'application/octet-stream'}")
        
        with col2:
            st.write("**Options:**")
            save_to_disk = st.checkbox("Save to disk", value=True)
            calculate_hash = st.checkbox("Calculate SHA-256 hash", value=True)
        
        if st.button("Process File", type="primary"):
            with st.spinner("Processing file..."):
                try:
                    # Save to disk
                    if save_to_disk:
                        temp_path = save_uploaded_file_to_disk(uploaded_file)
                        st.success(f"‚úÖ File saved to: {temp_path}")
                        
                        # Calculate hash
                        if calculate_hash:
                            hash_value = calculate_hash_file(temp_path, 'sha256')
                            st.code(hash_value, language="text")
                            st.caption("SHA-256 Hash")
                        
                        # Show next steps
                        st.info("""
                        **Next Steps:**
                        - For forensic analysis, return to the main dashboard
                        - To upload to S3, use the Direct S3 Upload mode for larger files
                        - For case management, associate this file with a case ID
                        """)
                    
                except Exception as e:
                    st.error(f"‚ùå Error processing file: {str(e)}")


def render_help_section():
    """Render help and documentation section"""
    st.divider()
    
    with st.expander("üìñ Help & Documentation"):
        st.markdown("""
        ### About Large File Uploads
        
        **Why Direct S3 Upload?**
        - Streamlit has memory limits (~50 GB maxUploadSize in config)
        - Traditional uploads route entire file through server memory
        - For 1 TB files, this is not feasible
        - Direct S3 upload bypasses the server entirely
        
        **How It Works:**
        1. **Multipart Upload**: Large files are split into parts (5 MB - 5 GB each)
        2. **Presigned URLs**: Temporary URLs grant upload permission to S3
        3. **Browser Upload**: JavaScript uploads parts directly from browser to S3
        4. **Completion**: After all parts are uploaded, they're assembled in S3
        
        **Requirements:**
        1. **FastAPI Backend**: Run `server/s3_complete_app.py`
           ```bash
           uvicorn server.s3_complete_app:app --host 0.0.0.0 --port 8001
           ```
        
        2. **AWS Credentials**: Set environment variables or use IAM role
           ```bash
           export AWS_ACCESS_KEY_ID=your_key
           export AWS_SECRET_ACCESS_KEY=your_secret
           export AWS_DEFAULT_REGION=us-east-1
           ```
        
        3. **S3 Bucket**: Create bucket and configure CORS
           ```json
           [
             {
               "AllowedHeaders": ["*"],
               "AllowedMethods": ["GET", "PUT", "POST"],
               "AllowedOrigins": ["http://localhost:8501"],
               "ExposeHeaders": ["ETag"]
             }
           ]
           ```
        
        4. **IAM Permissions**: Grant s3:PutObject, s3:GetObject, etc.
        
        **Security Considerations:**
        - Presigned URLs are temporary (1 hour by default)
        - Only provide URLs to authenticated users
        - Use HTTPS in production
        - Implement rate limiting
        - Monitor S3 access logs
        
        **Troubleshooting:**
        - **Backend not reachable**: Ensure FastAPI service is running
        - **CORS errors**: Check S3 bucket CORS configuration
        - **Permission denied**: Verify IAM permissions
        - **Upload fails**: Check browser console for errors
        
        See README.md for complete documentation.
        """)


def main():
    """Main page entry point"""
    
    # Page header
    st.title("üì§ Large File Upload")
    st.caption("Upload files up to 1 TB directly to S3")
    
    # Render main content
    render_s3_direct_upload_demo()
    
    # Render help section
    render_help_section()
    
    # Footer
    st.divider()
    st.caption("¬© 2025 CORTEX - Mobile Device Forensics Analyzer")


if __name__ == "__main__":
    main()
